{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import torch\n",
    "\n",
    "from OFDM_SDR_Functions_torch import *\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as tFunc # usually F, but that is reserved for other use\n",
    "import csv\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "# Display the selected device\n",
    "print(device)\n",
    "\n",
    "# Test tensor creation on the selected device\n",
    "if device.type != \"cpu\":\n",
    "    x = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "\n",
    "#device = \"cpu\" # Force CPU for now, trouble with converting complex tensors to mps with macos M1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Load dataset created earlier and make the torch dataloaders. The dataset structure is defined in the config.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataset = torch.load('data/ofdm_dataset.pth')\n",
    "batch_size = 16\n",
    "\n",
    "# train, validation and test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set= torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN-receiver model\n",
    "Create a torch model for the receiver. The structure follows loosely the original DeepRX (https://arxiv.org/abs/2005.01494) structure, but is simplified and lighter. The model architecture is stored in `models_local.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_local import *\n",
    "model = RXModel_2(Qm).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Set the optimizer and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "initial_lr = 0.001 # Initial learning rate\n",
    "final_lr = 0.0003 # Final learning rate at the end\n",
    "num_epochs = 100 # epochs for learning rate scheduler decay\n",
    "\n",
    "# Define the model's optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# Lambda function for learning rate decay\n",
    "lambda_lr = lambda epoch: final_lr / initial_lr + (1 - epoch / num_epochs) * (1 - final_lr / initial_lr)\n",
    "\n",
    "# Define the learning rate scheduler and loss function\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load earlier model for further training or start from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'data/rx_model_168.pth'\n",
    "performance_csv_path = 'data/performance_details.csv'\n",
    "\n",
    "# Check if a saved model exists\n",
    "if os.path.exists(saved_model_path):\n",
    "    # Load the existing model and epoch\n",
    "    checkpoint = torch.load(saved_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Existing model loaded from {saved_model_path}, Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved model found. Training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model performance data CSV for observing the performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store performance details for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_BERs = []\n",
    "\n",
    "# Check if a performance CSV file exists\n",
    "if not os.path.exists(performance_csv_path):\n",
    "    # Create a new CSV file and write headers\n",
    "    with open(performance_csv_path, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['Epoch', 'Training_Loss', 'Validation_Loss', 'Validation_BER']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTI_mask_RE = TTI_mask(S=S,F=F, Fp=Fp, Sp=Sp, FFT_offset=FFT_offset, plotTTI=False).to(device) # TTI mask\n",
    "pilot_symbols = pilot_set(TTI_mask_RE, Pilot_Power).to(device) # pilot symbols\n",
    "\n",
    "# remove DC and FFT offsets from TTI mask_RE and add third dimension size of Qm, and expand TTI mask values into the third dimension\n",
    "TTI_mask_RE_small = TTI_mask_RE[:, FFT_offset:-FFT_offset]\n",
    "middle_index = TTI_mask_RE_small.size(1) // 2\n",
    "TTI_mask_RE_small = torch.cat((TTI_mask_RE_small[:, :middle_index], TTI_mask_RE_small[:, middle_index + 1:]), dim=1)\n",
    "\n",
    "TTI_mask_RE_3d = TTI_mask_RE_small.unsqueeze(-1).expand(batch_size, S, F-1, Qm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for pdsch_iq,  labels in train_loader:\n",
    "        pdsch_iq,  labels = pdsch_iq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model((pdsch_iq))  # forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # backward pass\n",
    "        optimizer.step()  # update the weights\n",
    "        total_loss += loss.item()  # accumulate the loss\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_pdsch_iq, val_labels in val_loader:\n",
    "            val_pdsch_iq, val_labels = val_pdsch_iq.to(device), val_labels.to(device)\n",
    "            val_outputs = model((val_pdsch_iq))\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            # Convert probabilities to binary predictions (0 or 1)\n",
    "            binary_predictions = torch.round(val_outputs)\n",
    "\n",
    "            # Calculate Bit Error Rate (BER)\n",
    "            error_count = torch.sum(binary_predictions != val_labels).float()  # Count of unequal bits\n",
    "            error_rate = error_count / len(val_labels.flatten())  # Error rate calculation\n",
    "            BER = torch.round(error_rate * 1000) / 1000  # Round to 3 decimal places\n",
    "\n",
    "    model.train()  # Set the model back to training mode\n",
    "\n",
    "    # Save performance details\n",
    "    train_losses.append(average_loss)\n",
    "    val_losses.append(val_loss.item())\n",
    "    val_BERs.append(BER.item())\n",
    "\n",
    "    # Print or log validation loss after each epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}, Val Loss: {val_loss:.4f}, Val BER: {BER:.4f}, learning rate: {scheduler.get_last_lr()[0]:.4f}\")\n",
    "\n",
    "    # Save performance details in the CSV file\n",
    "    with open(performance_csv_path, mode='a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([epoch + 1, average_loss, val_loss.item(), BER.item()])\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        # Save model along with the current epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, f'data/rx_model_{epoch + 1}.pth')\n",
    "        print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Save the final trained model\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, 'data/rx_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch [1/50], Loss: 0.6828, Val Loss: 0.6734, Val BER: 0.4510, learning rate: 0.0010\n",
    "Epoch [2/50], Loss: 0.3327, Val Loss: 0.0729, Val BER: 0.0310, learning rate: 0.0010\n",
    "Model saved at epoch 2\n",
    "Epoch [3/50], Loss: 0.0478, Val Loss: 0.0294, Val BER: 0.0140, learning rate: 0.0009\n",
    "Epoch [4/50], Loss: 0.0313, Val Loss: 0.0369, Val BER: 0.0170, learning rate: 0.0009\n",
    "Model saved at epoch 4\n",
    "Epoch [5/50], Loss: 0.0244, Val Loss: 0.0228, Val BER: 0.0100, learning rate: 0.0009\n",
    "Epoch [6/50], Loss: 0.0173, Val Loss: 0.0218, Val BER: 0.0090, learning rate: 0.0009\n",
    "Model saved at epoch 6\n",
    "Epoch [7/50], Loss: 0.0136, Val Loss: 0.0060, Val BER: 0.0020, learning rate: 0.0009\n",
    "Epoch [8/50], Loss: 0.0118, Val Loss: 0.0144, Val BER: 0.0060, learning rate: 0.0009\n",
    "Model saved at epoch 8\n",
    "Epoch [9/50], Loss: 0.0109, Val Loss: 0.0216, Val BER: 0.0090, learning rate: 0.0008\n",
    "Epoch [10/50], Loss: 0.0098, Val Loss: 0.0173, Val BER: 0.0060, learning rate: 0.0008\n",
    "Model saved at epoch 10\n",
    "Epoch [11/50], Loss: 0.0093, Val Loss: 0.0010, Val BER: 0.0000, learning rate: 0.0008\n",
    "Epoch [12/50], Loss: 0.0085, Val Loss: 0.0039, Val BER: 0.0010, learning rate: 0.0008\n",
    "Model saved at epoch 12\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance during training\n",
    "Visualize the performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV data\n",
    "csv_path = 'data/performance_details.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Plot Training Loss and Validation Loss\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(df['Epoch'], df['Training_Loss'], label='Training Loss')\n",
    "plt.plot(df['Epoch'], df['Validation_Loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "if save_plots:\n",
    "    plt.savefig('pics/training_loss.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot Validation BER\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(df['Epoch'], df['Validation_BER'], label='Validation BER')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('BER')\n",
    "plt.legend()\n",
    "plt.title('Bit Error Rate (BER) on validation set')\n",
    "plt.grid(True)\n",
    "plt.ylim(0,0.06)\n",
    "if save_plots:\n",
    "    plt.savefig('pics/training_ber.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pluto311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
