{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import torch\n",
    "\n",
    "from OFDM_SDR_Functions_torch import *\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as tFunc # usually F, but that is reserved for other use\n",
    "import csv\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "# Display the selected device\n",
    "print(device)\n",
    "\n",
    "# Test tensor creation on the selected device\n",
    "if device.type != \"cpu\":\n",
    "    x = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "\n",
    "device = \"cpu\" # Force CPU for now, trouble with converting complex tensors to mps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Load dataset created earlier and make the torch dataloaders. The dataset structure is defined in the config.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataset = torch.load('data/ofdm_dataset.pth')\n",
    "saved_model_path = 'data/rx_model.pth'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# train, validation and test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set= torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN-receiver model\n",
    "Create a torch model for the receiver. The structure follows loosely the DeepRX (https://arxiv.org/abs/2005.01494) structure, but is simplified and lighter. The model architecture is stored in `models_local.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_local import *\n",
    "\n",
    "model = RXModel(Qm).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Set the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load earlier model for further training or start from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'data/rx_model_40.pth'\n",
    "performance_csv_path = 'data/performance_details.csv'\n",
    "num_epochs = 200\n",
    "\n",
    "# Check if a saved model exists\n",
    "if os.path.exists(saved_model_path):\n",
    "    # Load the existing model and epoch\n",
    "    checkpoint = torch.load(saved_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Existing model loaded from {saved_model_path}, Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print(\"No saved model found. Training from scratch.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store performance details for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_BERs = []\n",
    "\n",
    "# Check if a performance CSV file exists\n",
    "if not os.path.exists(performance_csv_path):\n",
    "    # Create a new CSV file and write headers\n",
    "    with open(performance_csv_path, mode='w', newline='') as csv_file:\n",
    "        fieldnames = ['Epoch', 'Training Loss', 'Validation Loss', 'Validation BER']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for pdsch_iq, pilot_iq, labels in train_loader:\n",
    "        pdsch_iq, pilot_iq, labels = pdsch_iq.to(device), pilot_iq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model((pdsch_iq, pilot_iq))  # forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # backward pass\n",
    "        optimizer.step()  # update the weights\n",
    "        total_loss += loss.item()  # accumulate the loss\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_pdsch_iq, val_pilot_iq, val_labels in val_loader:\n",
    "            val_pdsch_iq, val_pilot_iq, val_labels = val_pdsch_iq.to(device), val_pilot_iq.to(device), val_labels.to(device)\n",
    "            val_outputs = model((val_pdsch_iq, val_pilot_iq))\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            # Convert probabilities to binary predictions (0 or 1)\n",
    "            binary_predictions = torch.round(val_outputs)\n",
    "\n",
    "            # Calculate Bit Error Rate (BER)\n",
    "            error_count = torch.sum(binary_predictions != val_labels).float()  # Count of unequal bits\n",
    "            error_rate = error_count / len(val_labels.flatten())  # Error rate calculation\n",
    "            BER = torch.round(error_rate * 1000) / 1000  # Round to 3 decimal places\n",
    "\n",
    "    model.train()  # Set the model back to training mode\n",
    "\n",
    "    # Save performance details\n",
    "    train_losses.append(average_loss)\n",
    "    val_losses.append(val_loss.item())\n",
    "    val_BERs.append(BER.item())\n",
    "\n",
    "    # Print or log validation loss after each epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}, Val Loss: {val_loss:.4f}, Val BER: {BER:.4f}\")\n",
    "\n",
    "    # Save performance details in the CSV file\n",
    "    with open(performance_csv_path, mode='a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([epoch + 1, average_loss, val_loss.item(), BER.item()])\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Save model along with the current epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, f'data/rx_model_{epoch + 1}.pth')\n",
    "        print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "# Save the final trained model\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, 'data/rx_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch [1/200], Loss: 0.6900, Val Loss: 0.6770, Val BER: 0.4820\n",
    "Epoch [2/200], Loss: 0.6769, Val Loss: 0.6690, Val BER: 0.4590\n",
    "Epoch [3/200], Loss: 0.4725, Val Loss: 0.3565, Val BER: 0.2150\n",
    "Epoch [4/200], Loss: 0.2784, Val Loss: 0.2097, Val BER: 0.1060\n",
    "Epoch [5/200], Loss: 0.1978, Val Loss: 0.1299, Val BER: 0.0620\n",
    "Model saved at epoch 5\n",
    "Epoch [6/200], Loss: 0.1658, Val Loss: 0.1788, Val BER: 0.0850\n",
    "Epoch [7/200], Loss: 0.0978, Val Loss: 0.0792, Val BER: 0.0370\n",
    "Epoch [8/200], Loss: 0.1464, Val Loss: 0.1386, Val BER: 0.0660\n",
    "Epoch [9/200], Loss: 0.1023, Val Loss: 0.0720, Val BER: 0.0330\n",
    "Epoch [10/200], Loss: 0.1284, Val Loss: 0.1032, Val BER: 0.0490\n",
    "Model saved at epoch 10\n",
    "Epoch [11/200], Loss: 0.0843, Val Loss: 0.0798, Val BER: 0.0360\n",
    "Epoch [12/200], Loss: 0.1543, Val Loss: 0.1698, Val BER: 0.0820\n",
    "Epoch [13/200], Loss: 0.0941, Val Loss: 0.0725, Val BER: 0.0320\n",
    "Epoch [14/200], Loss: 0.1259, Val Loss: 0.0796, Val BER: 0.0350\n",
    "Epoch [15/200], Loss: 0.0686, Val Loss: 0.0867, Val BER: 0.0370\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance during training\n",
    "Visualize the performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV data\n",
    "csv_path = 'data/performance_details.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Plot Training Loss and Validation Loss\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(df['Epoch'], df['Training Loss'], label='Training Loss')\n",
    "plt.plot(df['Epoch'], df['Validation Loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.savefig('pics/training_loss.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot Validation BER\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(df['Epoch'], df['Validation BER'], label='Validation BER')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('BER')\n",
    "plt.legend()\n",
    "plt.title('Bit Error Rate (BER) on validation set')\n",
    "plt.grid(True)\n",
    "plt.savefig('pics/training_ber.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the training and validation data exhibit deep frequency-selective dips and additional noise, preventing the validation Bit Error Rate (BER) from reaching 0. Nevertheless, the model demonstrates effective learning in decoding the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pluto311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
