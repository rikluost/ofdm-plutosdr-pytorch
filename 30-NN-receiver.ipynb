{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural receiver, training and testing it \n",
    "\n",
    "### WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
    "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=[128, None, None])  # Replace None with specific time and frequency dimensions if known\n",
    "        self.conv_1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding='same')\n",
    "        \n",
    "        self.layer_norm_2 = nn.LayerNorm(normalized_shape=[128, None, None])  # Replace None with specific time and frequency dimensions if known\n",
    "        self.conv_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding='same')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z = self.layer_norm_1(inputs)\n",
    "        z = F.relu(z)\n",
    "        z = self.conv_1(z)\n",
    "        z = self.layer_norm_2(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.conv_2(z)\n",
    "        # Skip connection\n",
    "        z += inputs\n",
    "\n",
    "        return z\n",
    "\n",
    "class NeuralReceiver(nn.Module):\n",
    "\n",
    "    def __init__(self, num_bits_per_symbol):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_conv = nn.Conv2d(in_channels=2, out_channels=128, kernel_size=(3, 3), padding='same')\n",
    "        self.res_block_1 = ResidualBlock()\n",
    "        self.res_block_2 = ResidualBlock()\n",
    "        self.res_block_3 = ResidualBlock()\n",
    "        self.res_block_4 = ResidualBlock()\n",
    "        self.output_conv = nn.Conv2d(in_channels=128, out_channels=num_bits_per_symbol, kernel_size=(3, 3), padding='same')\n",
    "\n",
    "    def forward(self, y, no):\n",
    "        # Assuming a single receiver, remove the num_rx dimension\n",
    "        y = y.squeeze(dim=1)\n",
    "\n",
    "        # Feeding the noise power in log10 scale helps with the performance\n",
    "        no = torch.log10(no)\n",
    "\n",
    "        # Stacking the real and imaginary components of the different antennas along the 'channel' dimension\n",
    "        y = y.permute(0, 2, 3, 1)  # Putting antenna dimension last\n",
    "        no = no.unsqueeze(3).expand(-1, y.shape[1], y.shape[2], -1)\n",
    "        z = torch.cat([torch.real(y), torch.imag(y), no], dim=-1)\n",
    "        z = self.input_conv(z)\n",
    "        z = self.res_block_1(z)\n",
    "        z = self.res_block_2(z)\n",
    "        z = self.res_block_3(z)\n",
    "        z = self.res_block_4(z)\n",
    "        z = self.output_conv(z)\n",
    "\n",
    "        # Reshape the input to fit what the resource grid demapper is expected\n",
    "        # Add a dimension at position 2, if needed\n",
    "        # z = z.unsqueeze(2)\n",
    "\n",
    "        return z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pluto311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
